# -*- coding: utf-8 -*-
import os.path
import scrapy
from scrapy import Request
from scrapy_splash import SplashRequest
from tkitreadability import tkitReadability
import time
# from ..items import *
from scrapy_redis.spiders import RedisSpider
from bs4 import BeautifulSoup as bs

"""
use splash
terrybasic_splash.tmpl
当前文件需要放入python包所在地，
比如site-packages/scrapy/templates/spiders/。
https://github.com/scrapy-plugins/scrapy-splash
use it
> scrapy genspider -t terrybasic_splash  test test.com

"""

class ${classname}Item(scrapy.Item):
    """
    内容基本,can move to items.py

    """
    # define the fields for your item here like:
    mongodb_spiders = ['$name']
    # test it first
    #mongodb_collections = ['content']
    mongodb_collections = ['test_content']

    content_type = scrapy.Field()
    title=scrapy.Field()
    content=scrapy.Field()
    content_html=scrapy.Field()
    url=scrapy.Field()
    category=scrapy.Field()
    image_urls=scrapy.Field()
    images=scrapy.Field()
    ex=scrapy.Field()
    pass

# class $classname(scrapy.Spider):
class $classname(RedisSpider):
    """$classname

    """

    name = '$name'
    allowed_domains = ['$domain']
    start_urls = ['https://$domain/']
    base_url = "https://$domain"
    img_url = "https://$domain"

    # 这里覆盖默认的settings配置,custom_settings settings.py
    custom_settings = dict(
        #MONGODB_URI="mongodb://192.168.1.18:27017/",
        #REDIS_URL = 'redis://192.168.1.18:6379',  # 指定redis的地址
        #POOL_PROXY = "http://192.168.1.18:5010"
        SPLASH_URL = 'http://192.168.1.18:8050',
        DOWNLOADER_MIDDLEWARES = {
            'scrapy_splash.SplashCookiesMiddleware': 723,
            'scrapy_splash.SplashMiddleware': 725,
            'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
        },

        # 去重复
        SPIDER_MIDDLEWARES={
            "scrapy_splash.SplashDeduplicateArgsMiddleware": 100,
            "scrapy_redis.dupefilter.RFPDupeFilter":101
        },
        #DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter',
        HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage',
        CONCURRENT_REQUESTS_PER_IP = 4,
        CONCURRENT_REQUESTS_PER_DOMAIN = 4

    )
    redis_key = f'{name}:start_urls'  # 即启动爬虫的命令，参考格式：redis_key =
    def start_requests(self):
        for url in self.start_urls:
            yield SplashRequest(url, self.parse, args={'wait': 0.5})

    def parse(self, response):
        """
        解析链接

        :param response:
        :return:
        """
        for item in response.xpath('//a/@href').extract():
            # print(item)
            if item.startswith("http"):
                url = item
                pass
            elif item.startswith("tel:") or item.startswith("mailto:"):
                continue
            else:
                url = os.path.join(self.base_url, item)
            # if "articles" in url:
            # 解析内容
            yield SplashRequest(
                url,
                self.parse_content,
                args={
                    # optional; parameters passed to Splash HTTP API
                    'wait': 0.5,
                    # 'url': url

                    # 'url' is prefilled from request url
                    # 'http_method' is set to 'POST' for POST requests
                    # 'body' is set to request body for POST requests
                },
                # endpoint='render.json', # optional; default is render.html
                # splash_url='<url>',     # optional; overrides SPLASH_URL
                # slot_policy=scrapy_splash.SlotPolicy.PER_DOMAIN,  # optional
            )


    def parse_content(self, response):
        """
        解析内容

        :param response:
        :return:
        """
        # 断言内容是文本
        assert response.text
        content_limit=128

        # 标题
        # content_type=self.name
        xpath_title='//h1[@class="hero-title"]'
        # xpath_content='//div[contains(@class,"article--main-content-wrapper")]/div[@class="node__contentr"]'
        xpath_content='//article//div[contains(@class,"entry-content")]'

        xpath_content_img=None

        # need remove items
        xpath_removes=[]
        # 删除表格
        #xpath_removes.append('//body[contains(@class,"node-article")]//article//div[contains(@class,"articleContainer__content")]//div[contains(@class,"component") and contains(@class,"table-of-contents")]')


        # save to items
        items = ${classname}Item()

        if len(response.xpath(xpath_title))==1 and len(response.xpath(xpath_content))>0:
            # 使用同义的数据格式

            # 修改爬虫名称
            # items.mongodb_spiders = [self.name]
            items['content_type'] = self.name
            # items["title"] = response.xpath(f'{xpath_title}/text()').extract()[0]
            items["title"] = response.xpath(f'{xpath_title}/text()').get()
            for xpath_remove in xpath_removes:
                response.xpath(xpath_remove).remove()
            # 使用自动正则方案获取正文
            # html = response.xpath(xpath_content).extract()[0]
            html = response.xpath(xpath_content).get()

            # 修改html内容
            #soup=bs(html, 'html.parser')
            #for img in soup.findAll('img'):
            #    #修正图片地址
            #    if 'data-load-src' in img.attrs.keys():
            #        img['src'] = img.attrs['data-load-src']
            #html=str(soup)
            # 修正html内容结束

            Readability = tkitReadability()
            content = Readability.html2markdown(html,images_with_size=False)
            items["content"] = content

            if len(items["content"].split(" ")) > content_limit:

                # 收集全部数据。带有html
                items["content_html"] = html

                srcs = []
                # image source
                if xpath_content_img is not None:
                    for s in response.xpath(f'{xpath_content_img}//img/@src').extract():
                        if s.startswith('http') or s.startswith("data:image"):
                            srcs.append(s)
                        else:
                            # print(self.img_url+s)
                            srcs.append(self.img_url + s)

                for s in response.xpath(f'{xpath_content}//img/@src').extract():
                    if s.startswith('http') or s.startswith("data:image"):
                        srcs.append(s)
                    else:
                        # print(self.img_url+s)
                        srcs.append(self.img_url + s)

                items["image_urls"] = list(set(srcs))
                items["url"] = response.url
                yield items


        # print("解析内容")
        # print(response.xpath('/html').extract())
        for url in response.xpath('//a/@href').extract():
            if url.startswith("http"):
                pass
            elif url.startswith("tel:") or url.startswith("mailto:"):
                continue
            else:
                url = os.path.join(self.base_url, url)

            # yield response.follow(url, callback=self.parse_content)
            yield SplashRequest(
                url,
                self.parse_content,
                args={
                    # optional; parameters passed to Splash HTTP API
                    'wait': 0.5,
                    # 'url': url

                    # 'url' is prefilled from request url
                    # 'http_method' is set to 'POST' for POST requests
                    # 'body' is set to request body for POST requests
                },
                # endpoint='render.json', # optional; default is render.html
                # splash_url='<url>',     # optional; overrides SPLASH_URL
                # slot_policy=scrapy_splash.SlotPolicy.PER_DOMAIN,  # optional
            )


